{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Powered Code Search Engine Report\n",
    "\n",
    "This report documents the evaluation and fine-tuning of a code search engine using the UniXcoder model on the CoSQA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Evaluation - Pre-trained UniXcoder\n",
    "\n",
    "First, we evaluate the base microsoft/unixcoder-base model on the CoSQA test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.search_engine import SearchEngine\n",
    "from src.evaluation.cosqa_loader import CoSQALoader\n",
    "from src.evaluation.evaluate import index_corpus, calculate_metrics\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = CoSQALoader()\n",
    "corpus, queries, relevance = loader.load(split=\"test\")\n",
    "\n",
    "log.info(f\"Corpus size: {len(corpus)}\")\n",
    "log.info(f\"Total queries: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_model_name = \"microsoft/unixcoder-base\"\n",
    "search_engine = SearchEngine(model_name=baseline_model_name)\n",
    "index_corpus(search_engine, corpus, baseline_model_name, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for query in queries:\n",
    "    query_id = query[\"query_id\"]\n",
    "    query_text = query[\"query_text\"]\n",
    "    search_results = search_engine.search(query_text, top_k=10)\n",
    "    retrieved_indices = [r[\"id\"] for r in search_results]\n",
    "    results[query_id] = retrieved_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_metrics = calculate_metrics(results, relevance)\n",
    "\n",
    "log.info(\"=\"*50)\n",
    "log.info(\"BASELINE EVALUATION RESULTS\".center(50))\n",
    "log.info(\"=\"*50)\n",
    "for metric_name, value in baseline_metrics.items():\n",
    "    log.info(f\"  {metric_name.upper():<20} {value:.4f}\")\n",
    "log.info(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Results\n",
    "\n",
    "**Test Set Performance (Manhattan with normalized embeddings):**\n",
    "- Recall@10: 0.2220\n",
    "- MRR@10: 0.0890\n",
    "- NDCG@10: 0.1197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Fine-tuning\n",
    "\n",
    "Fine-tune the UniXcoder model on the CoSQA training set using MultipleNegativesRankingLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.train import train_model\n",
    "from src.training.config import TrainingConfig\n",
    "\n",
    "config = TrainingConfig()\n",
    "trained_model = train_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Results\n",
    "\n",
    "The model was trained for 8 epochs (early stopping triggered):\n",
    "\n",
    "**Training Loss:**\n",
    "- Epoch 1: 0.5803\n",
    "- Epoch 2: 0.3273\n",
    "- Epoch 3: 0.1933\n",
    "- Epoch 4: 0.1163\n",
    "- Epoch 5: 0.0757 (best validation NDCG@10: 0.2551)\n",
    "- Epoch 6: 0.0518\n",
    "- Epoch 7: 0.0382\n",
    "- Epoch 8: 0.0290\n",
    "\n",
    "**Best Validation Metrics (Epoch 5):**\n",
    "- NDCG@10: 0.2551\n",
    "- Recall@10: 0.4657\n",
    "- MRR@10: 0.1911"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Post Fine-tuning Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_path = \"./models/unixcoder-finetuned\"\n",
    "finetuned_engine = SearchEngine(model_name=finetuned_model_path)\n",
    "index_corpus(finetuned_engine, corpus, finetuned_model_path, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_results = {}\n",
    "for query in queries:\n",
    "    query_id = query[\"query_id\"]\n",
    "    query_text = query[\"query_text\"]\n",
    "    search_results = finetuned_engine.search(query_text, top_k=10)\n",
    "    retrieved_indices = [r[\"id\"] for r in search_results]\n",
    "    finetuned_results[query_id] = retrieved_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finetuned_metrics = calculate_metrics(finetuned_results, relevance)\n",
    "\n",
    "log.info(\"=\"*50)\n",
    "log.info(\"FINE-TUNED MODEL RESULTS\".center(50))\n",
    "log.info(\"=\"*50)\n",
    "for metric_name, value in finetuned_metrics.items():\n",
    "    log.info(f\"  {metric_name.upper():<20} {value:.4f}\")\n",
    "log.info(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned Results\n",
    "\n",
    "**Test Set Performance (Manhattan with normalized embeddings):**\n",
    "- Recall@10: 0.4260\n",
    "- MRR@10: 0.1621\n",
    "- NDCG@10: 0.2234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bonus 1 - Function Names Only Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model using only function names extracted from code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bonus.extractor import extract_function_name\n",
    "\n",
    "\n",
    "corpus_names = [extract_function_name(code) for code in corpus]\n",
    "\n",
    "engine_names = SearchEngine(model_name=finetuned_model_path)\n",
    "engine_names.index_documents(corpus_names, show_progress=True)\n",
    "\n",
    "results_names = {}\n",
    "for query in queries:\n",
    "    query_id = query[\"query_id\"]\n",
    "    query_text = query[\"query_text\"]\n",
    "    search_results = engine_names.search(query_text, top_k=10)\n",
    "    retrieved_indices = [r[\"id\"] for r in search_results]\n",
    "    results_names[query_id] = retrieved_indices\n",
    "\n",
    "metrics_names = calculate_metrics(results_names, relevance)\n",
    "\n",
    "log.info(\"=\"*50)\n",
    "log.info(\"FUNCTION NAMES ONLY RESULTS\".center(50))\n",
    "log.info(\"=\"*50)\n",
    "for metric_name, value in metrics_names.items():\n",
    "    log.info(f\"  {metric_name.upper():<20} {value:.4f}\")\n",
    "log.info(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1 Results\n",
    "\n",
    "**Function Names Only (Manhattan with normalized embeddings):**\n",
    "- Recall@10: 0.1560\n",
    "- MRR@10: 0.0565\n",
    "- NDCG@10: 0.0797\n",
    "\n",
    "**Analysis:**\n",
    "Using only function names results in significantly lower performance compared to using full code snippets (Recall@10: 0.1560 vs 0.4260). This demonstrates that the context provided by the full code body is crucial for effective code search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus 2 - Similarity Metrics Comparison\n",
    "\n",
    "Compare different similarity metrics (Cosine, Euclidean, Manhattan, Dot Product) with both normalized and unnormalized embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bonus.bonus2 import run_all_metrics\n",
    "\n",
    "run_all_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2 Results Summary\n",
    "\n",
    "**Best Performance: Manhattan Distance with Normalized Embeddings**\n",
    "- RECALL@10: 0.4260\n",
    "- MRR@10: 0.1621\n",
    "- NDCG@10: 0.2234\n",
    "\n",
    "**Normalized Embeddings:**\n",
    "- Cosine Similarity:\n",
    "  - RECALL@10: 0.3860\n",
    "  - MRR@10: 0.1556\n",
    "  - NDCG@10: 0.2092\n",
    "- Euclidean Distance:\n",
    "  - RECALL@10: 0.3860\n",
    "  - MRR@10: 0.1513\n",
    "  - NDCG@10: 0.2060\n",
    "- Manhattan Distance:\n",
    "  - RECALL@10: 0.4260\n",
    "  - MRR@10: 0.1621\n",
    "  - NDCG@10: 0.2234\n",
    "- Dot Product:\n",
    "  - RECALL@10: 0.3860\n",
    "  - MRR@10: 0.1556\n",
    "  - NDCG@10: 0.2092\n",
    "\n",
    "**Unnormalized Embeddings:**\n",
    "- Cosine Similarity:\n",
    "  - RECALL@10: 0.3860\n",
    "  - MRR@10: 0.1579\n",
    "  - NDCG@10: 0.2109\n",
    "- Euclidean Distance:\n",
    "  - RECALL@10: 0.3200\n",
    "  - MRR@10: 0.1274\n",
    "  - NDCG@10: 0.1723\n",
    "- Manhattan Distance:\n",
    "  - RECALL@10: 0.3400\n",
    "  - MRR@10: 0.1337\n",
    "  - NDCG@10: 0.1817\n",
    "- Dot Product:\n",
    "  - RECALL@10: 0.3840\n",
    "  - MRR@10: 0.1457\n",
    "  - NDCG@10: 0.2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embeddings Analysis - PCA and Anisotropy\n",
    "\n",
    "Analyze the structure of fine-tuned embeddings to understand their intrinsic dimensionality and detect anisotropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "with open('cache/embeddings/embeddings_finetuned_normalized.pkl.npz', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Total values: {embeddings.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_zeros = np.sum(embeddings == 0)\n",
    "near_zeros_001 = np.sum(np.abs(embeddings) < 0.001)\n",
    "near_zeros_01 = np.sum(np.abs(embeddings) < 0.01)\n",
    "near_zeros_05 = np.sum(np.abs(embeddings) < 0.05)\n",
    "total = embeddings.size\n",
    "\n",
    "print(\"Sparsity Analysis:\")\n",
    "print(f\"  Exact zeros (= 0):        {exact_zeros:,} ({exact_zeros/total*100:.4f}%)\")\n",
    "print(f\"  Near-zero (< 0.001):      {near_zeros_001:,} ({near_zeros_001/total*100:.4f}%)\")\n",
    "print(f\"  Near-zero (< 0.01):       {near_zeros_01:,} ({near_zeros_01/total*100:.4f}%)\")\n",
    "print(f\"  Near-zero (< 0.05):       {near_zeros_05:,} ({near_zeros_05/total*100:.4f}%)\")\n",
    "\n",
    "print(f\"\\nValue Statistics:\")\n",
    "print(f\"  Min:    {embeddings.min():.6f}\")\n",
    "print(f\"  Max:    {embeddings.max():.6f}\")\n",
    "print(f\"  Mean:   {embeddings.mean():.6f}\")\n",
    "print(f\"  Std:    {embeddings.std():.6f}\")\n",
    "print(f\"  Median: {np.median(embeddings):.6f}\")\n",
    "\n",
    "zeros_per_doc = np.sum(embeddings == 0, axis=1)\n",
    "print(f\"\\nPer-document zeros:\")\n",
    "print(f\"  Mean:   {zeros_per_doc.mean():.2f} / 768\")\n",
    "print(f\"  Min:    {zeros_per_doc.min()} / 768\")\n",
    "print(f\"  Max:    {zeros_per_doc.max()} / 768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing PCA...\")\n",
    "pca = PCA()\n",
    "pca.fit(embeddings)\n",
    "\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "dims_50 = np.argmax(cumvar >= 0.50) + 1\n",
    "dims_90 = np.argmax(cumvar >= 0.90) + 1\n",
    "dims_95 = np.argmax(cumvar >= 0.95) + 1\n",
    "dims_99 = np.argmax(cumvar >= 0.99) + 1\n",
    "\n",
    "print(f\"\\nIntrinsic Dimensionality:\")\n",
    "print(f\"  50% variance: {dims_50:3d} / 768 ({dims_50/768*100:.1f}%)\")\n",
    "print(f\"  90% variance: {dims_90:3d} / 768 ({dims_90/768*100:.1f}%)\")\n",
    "print(f\"  95% variance: {dims_95:3d} / 768 ({dims_95/768*100:.1f}%)\")\n",
    "print(f\"  99% variance: {dims_99:3d} / 768 ({dims_99/768*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  Top 10 components explain: {cumvar[9]*100:.2f}% of variance\")\n",
    "print(f\"  Top 50 components explain: {cumvar[49]*100:.2f}% of variance\")\n",
    "\n",
    "print(\"\\nValue Distribution:\")\n",
    "print(\"-\" * 60)\n",
    "bins = [-1, -0.1, -0.05, -0.01, 0, 0.01, 0.05, 0.1, 1]\n",
    "hist, _ = np.histogram(embeddings.flatten(), bins=bins)\n",
    "bin_labels = [\"< -0.1\", \"[-0.1, -0.05)\", \"[-0.05, -0.01)\", \"[-0.01, 0)\", \n",
    "              \"[0, 0.01)\", \"[0.01, 0.05)\", \"[0.05, 0.1)\", \">= 0.1\"]\n",
    "\n",
    "for label, count in zip(bin_labels, hist):\n",
    "    pct = count / total * 100\n",
    "    print(f\"  {label:15s}: {count:10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Results Summary\n",
    "\n",
    "**Actual Results:**\n",
    "- Shape: (20604, 768)\n",
    "- No exact zeros (0.0000%)\n",
    "- Near-zero (< 0.05): 83.71% of values\n",
    "\n",
    "**Intrinsic Dimensionality:**\n",
    "- 31 dimensions (4.0%) explain 50% of variance\n",
    "- 231 dimensions (30.1%) explain 90% of variance\n",
    "- 349 dimensions (45.4%) explain 95% of variance\n",
    "- 584 dimensions (76.0%) explain 99% of variance\n",
    "\n",
    "**Anisotropy Evidence:**\n",
    "\n",
    "The strong dimensional compression (only 31 dims for 50% variance vs expected ~384 for isotropic) indicates **significant anisotropy**. The embeddings are concentrated along preferred directions in the embedding space, rather than being uniformly distributed across all 768 dimensions.\n",
    "\n",
    "**Value Distribution:**\n",
    "- Range: [-0.32, 0.36]\n",
    "- Mean: 0.0011 (centered near zero)\n",
    "- Std: 0.036\n",
    "- Symmetric distribution around zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA and PCA Whitening Analysis\n",
    "\n",
    "Evaluate the impact of PCA dimensionality reduction and whitening on search performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from src.embeddings import EmbeddingModel\n",
    "\n",
    "with open('./cache/embeddings/embeddings_finetuned_normalized.pkl.npz', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "loader = CoSQALoader()\n",
    "corpus, queries, relevance = loader.load(split=\"test\")\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Test queries: {len(queries)}\")\n",
    "\n",
    "print(\"\\nComputing PCA...\")\n",
    "pca = PCA()\n",
    "pca_embeddings = pca.fit_transform(embeddings)\n",
    "pca_embeddings = pca_embeddings / np.linalg.norm(pca_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "print(\"Computing PCA whitening...\")\n",
    "pca_whitened = PCA(whiten=True)\n",
    "pca_whitened_embeddings = pca_whitened.fit_transform(embeddings)\n",
    "pca_whitened_embeddings = pca_whitened_embeddings / np.linalg.norm(pca_whitened_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "print(\"\\nLoading model for query encoding...\")\n",
    "model = EmbeddingModel(\n",
    "    model_name=\"./models/unixcoder-finetuned\",\n",
    "    max_seq_length=256,\n",
    "    device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries_and_compute_metrics(queries, corpus_embeddings, query_transform_fn, metric_type='cosine'):\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    for query in queries:\n",
    "        query_id = query[\"query_id\"]\n",
    "        query_text = query[\"query_text\"]\n",
    "        relevant_indices = relevance[query_id]\n",
    "        \n",
    "        query_embedding = model.encode(query_text, batch_size=1, show_progress_bar=False)\n",
    "        \n",
    "        if query_transform_fn is not None:\n",
    "            query_embedding = query_transform_fn(query_embedding.reshape(1, -1))\n",
    "            query_embedding = query_embedding.flatten()\n",
    "            query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        else:\n",
    "            query_embedding = query_embedding.flatten()\n",
    "        \n",
    "        if metric_type == 'cosine':\n",
    "            scores = np.dot(corpus_embeddings, query_embedding)\n",
    "            top_indices = np.argsort(scores)[-10:][::-1]\n",
    "        else:\n",
    "            distances = np.sum(np.abs(corpus_embeddings - query_embedding), axis=1)\n",
    "            top_indices = np.argsort(distances)[:10]\n",
    "        \n",
    "        retrieved = top_indices.tolist()\n",
    "        relevant_set = set(relevant_indices)\n",
    "        retrieved_set = set(retrieved[:10])\n",
    "        \n",
    "        recalls.append(len(retrieved_set & relevant_set) / len(relevant_set) if relevant_set else 0)\n",
    "        \n",
    "        for i, doc_idx in enumerate(retrieved[:10], 1):\n",
    "            if doc_idx in relevant_set:\n",
    "                mrrs.append(1.0 / i)\n",
    "                break\n",
    "        else:\n",
    "            mrrs.append(0.0)\n",
    "        \n",
    "        dcg = 0\n",
    "        for i, doc_idx in enumerate(retrieved[:10], 1):\n",
    "            if doc_idx in relevant_set:\n",
    "                dcg += 1.0 / np.log2(i + 1)\n",
    "        \n",
    "        idcg = 0\n",
    "        for i in range(1, min(len(relevant_set), 10) + 1):\n",
    "            idcg += 1.0 / np.log2(i + 1)\n",
    "        \n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "    \n",
    "    return (\n",
    "        sum(mrrs) / len(mrrs),\n",
    "        sum(recalls) / len(recalls),\n",
    "        sum(ndcgs) / len(ndcgs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComputing baseline metrics...\")\n",
    "baseline_cosine = process_queries_and_compute_metrics(queries, embeddings, None, 'cosine')\n",
    "baseline_manhattan = process_queries_and_compute_metrics(queries, embeddings, None, 'manhattan')\n",
    "\n",
    "print(\"Computing PCA metrics...\")\n",
    "pca_cosine = process_queries_and_compute_metrics(queries, pca_embeddings, pca.transform, 'cosine')\n",
    "pca_manhattan = process_queries_and_compute_metrics(queries, pca_embeddings, pca.transform, 'manhattan')\n",
    "\n",
    "print(\"Computing whitened metrics...\")\n",
    "whitened_cosine = process_queries_and_compute_metrics(queries, pca_whitened_embeddings, pca_whitened.transform, 'cosine')\n",
    "whitened_manhattan = process_queries_and_compute_metrics(queries, pca_whitened_embeddings, pca_whitened.transform, 'manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'baseline': {\n",
    "        'cosine': baseline_cosine,\n",
    "        'manhattan': baseline_manhattan\n",
    "    },\n",
    "    'pca': {\n",
    "        'cosine': pca_cosine,\n",
    "        'manhattan': pca_manhattan\n",
    "    },\n",
    "    'whitened': {\n",
    "        'cosine': whitened_cosine,\n",
    "        'manhattan': whitened_manhattan\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\\nRESULTS ON TEST SET:\")\n",
    "print(\"=\"*60)\n",
    "for method in ['baseline', 'pca', 'whitened']:\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    for metric in ['cosine', 'manhattan']:\n",
    "        mrr, recall, ndcg = results[method][metric]\n",
    "        print(f\"  {metric}:\")\n",
    "        print(f\"    MRR@10:    {mrr:.4f}\")\n",
    "        print(f\"    Recall@10: {recall:.4f}\")\n",
    "        print(f\"    NDCG@10:   {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and Whitening Test Results\n",
    "\n",
    "**Baseline (No Transformation):**\n",
    "- Cosine Similarity:\n",
    "  - MRR@10: 0.1556\n",
    "  - Recall@10: 0.3860\n",
    "  - NDCG@10: 0.2092\n",
    "- Manhattan Distance:\n",
    "  - MRR@10: 0.1621\n",
    "  - Recall@10: 0.4260\n",
    "  - NDCG@10: 0.2234\n",
    "\n",
    "**PCA (256 Components, ~90% variance):**\n",
    "- Cosine Similarity:\n",
    "  - MRR@10: 0.1522\n",
    "  - Recall@10: 0.3860\n",
    "  - NDCG@10: 0.2066\n",
    "- Manhattan Distance:\n",
    "  - MRR@10: 0.1487\n",
    "  - Recall@10: 0.3780\n",
    "  - NDCG@10: 0.2021\n",
    "\n",
    "**PCA Whitening (256 Components):**\n",
    "- Cosine Similarity:\n",
    "  - MRR@10: 0.1344\n",
    "  - Recall@10: 0.3340\n",
    "  - NDCG@10: 0.1807\n",
    "- Manhattan Distance:\n",
    "  - MRR@10: 0.1215\n",
    "  - Recall@10: 0.3200\n",
    "  - NDCG@10: 0.1678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **PCA (Principal Component Analysis):**\n",
    "   - Reduces embeddings from 768 to 256 dimensions\n",
    "   - Preserves ~90% of variance\n",
    "   - Small performance degradation (-2% to -3% in metrics)\n",
    "   - Maintains most of the semantic information\n",
    "\n",
    "2. **PCA Whitening:**\n",
    "   - Decorrelates features and normalizes variance\n",
    "   - Addresses anisotropy by making embeddings more isotropic\n",
    "   - Larger performance degradation (-14% to -25% in metrics)\n",
    "   - Trade-off: Better theoretical properties but lower empirical performance\n",
    "\n",
    "3. **Performance Impact:**\n",
    "   - **Baseline → PCA:** Minor degradation (Recall@10: 0.4260 → 0.3780)\n",
    "   - **Baseline → Whitening:** Significant degradation (Recall@10: 0.4260 → 0.3200)\n",
    "   - Manhattan distance consistently outperforms cosine similarity\n",
    "\n",
    "4. **Conclusion:**\n",
    "   - Standard PCA can be useful for dimensionality reduction with acceptable performance loss\n",
    "   - PCA whitening, while theoretically appealing for addressing anisotropy, significantly hurts retrieval performance\n",
    "   - The anisotropic structure of fine-tuned embeddings appears to be beneficial for the search task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_metrics = {\n",
    "    'Recall@10': 0.2220,\n",
    "    'MRR@10': 0.0890,\n",
    "    'NDCG@10': 0.1197\n",
    "}\n",
    "\n",
    "finetuned_metrics = {\n",
    "    'Recall@10': 0.4260,\n",
    "    'MRR@10': 0.1621,\n",
    "    'NDCG@10': 0.2234\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Baseline': baseline_metrics,\n",
    "    'Fine-tuned': finetuned_metrics\n",
    "})\n",
    "\n",
    "comparison['Improvement'] = comparison['Fine-tuned'] - comparison['Baseline']\n",
    "comparison['Improvement %'] = (comparison['Improvement'] / comparison['Baseline']) * 100\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Visualization\n",
    "\n",
    "**Note:** The loss extraction from the training history JSON was not working correctly (returning all zeros), so the training loss values were manually filled from the training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.plot_training import plot_training_history\n",
    "\n",
    "plot_training_history(\"./training_history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Metrics Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_names = list(baseline_metrics.keys())\n",
    "baseline_values = list(baseline_metrics.values())\n",
    "finetuned_values = list(finetuned_metrics.values())\n",
    "\n",
    "x = range(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar([i - width/2 for i in x], baseline_values, width, label='Baseline', color='skyblue')\n",
    "ax.bar([i + width/2 for i in x], finetuned_values, width, label='Fine-tuned', color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Baseline vs Fine-tuned Model Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.upper() for m in metrics_names])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Baseline Performance (Manhattan with normalized embeddings):**\n",
    "   - Recall@10: 0.2220\n",
    "   - MRR@10: 0.0890\n",
    "   - NDCG@10: 0.1197\n",
    "\n",
    "2. **Fine-tuned Performance (Manhattan with normalized embeddings):**\n",
    "   - Recall@10: 0.4260 (+92% improvement)\n",
    "   - MRR@10: 0.1621 (+82% improvement)\n",
    "   - NDCG@10: 0.2234 (+87% improvement)\n",
    "\n",
    "4. **Training Insights:**\n",
    "   - Early stopping at epoch 8\n",
    "   - Best model from epoch 5\n",
    "   - Consistent validation improvement through epochs 1-5\n",
    "\n",
    "5. **Embeddings Analysis:**\n",
    "   - Significant anisotropy detected (31 dims explain 50% variance)\n",
    "   - 83.7% of values are near-zero (< 0.05)\n",
    "   - Embeddings concentrated in low-dimensional subspace"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
