{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Powered Code Search Engine Report\n",
    "\n",
    "This report documents the evaluation and fine-tuning of a code search engine using the UniXcoder model on the CoSQA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Evaluation - Pre-trained UniXcoder\n",
    "\n",
    "First, we evaluate the base microsoft/unixcoder-base model on the CoSQA test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.search_engine import SearchEngine\n",
    "from evaluation.cosqa_loader import CoSQALoader\n",
    "from evaluation.metrics import recall_at_k, mrr_at_k, ndcg_at_k\n",
    "from evaluation.evaluate import get_cache_path, index_corpus, calculate_metrics\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "loader = CoSQALoader()\n",
    "corpus, queries, relevance = loader.load(split=\"test\")\n",
    "\n",
    "log.info(f\"Corpus size: {len(corpus)}\")\n",
    "log.info(f\"Total queries: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize baseline model\n",
    "baseline_model_name = \"microsoft/unixcoder-base\"\n",
    "search_engine = SearchEngine(model_name=baseline_model_name)\n",
    "index_corpus(search_engine, corpus, baseline_model_name, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results = {}\n",
    "for query in queries:\n",
    "    query_id = query[\"query_id\"]\n",
    "    query_text = query[\"query_text\"]\n",
    "    search_results = search_engine.search(query_text, top_k=10)\n",
    "    retrieved_indices = [r[\"id\"] for r in search_results]\n",
    "    results[query_id] = retrieved_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline metrics\n",
    "baseline_metrics = calculate_metrics(results, relevance)\n",
    "\n",
    "log.info(\"=\"*50)\n",
    "log.info(\"BASELINE EVALUATION RESULTS\".center(50))\n",
    "log.info(\"=\"*50)\n",
    "for metric_name, value in baseline_metrics.items():\n",
    "    log.info(f\"  {metric_name.upper():<20} {value:.4f}\")\n",
    "log.info(\"=\"*50)\n",
    "\n",
    "# Expected results:\n",
    "# RECALL@10: 0.2220\n",
    "# MRR@10: 0.0890\n",
    "# NDCG@10: 0.1197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Fine-tuning\n",
    "\n",
    "Fine-tune the UniXcoder model on the CoSQA training set using MultipleNegativesRankingLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.train import train_model\n",
    "from training.config import TrainingConfig\n",
    "\n",
    "# Train the model (this will take time)\n",
    "config = TrainingConfig()\n",
    "trained_model = train_model(config)\n",
    "\n",
    "# Training results saved to:\n",
    "# - Model: ./models/unixcoder-finetuned\n",
    "# - History: ./training_history.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Results\n",
    "\n",
    "The model was trained for 8 epochs (early stopping triggered):\n",
    "\n",
    "**Training Loss:**\n",
    "- Epoch 1: 0.5803\n",
    "- Epoch 2: 0.3273\n",
    "- Epoch 3: 0.1933\n",
    "- Epoch 4: 0.1163\n",
    "- Epoch 5: 0.0757 (best validation NDCG@10: 0.2551)\n",
    "- Epoch 6: 0.0518\n",
    "- Epoch 7: 0.0382\n",
    "- Epoch 8: 0.0290\n",
    "\n",
    "**Best Validation Metrics (Epoch 5):**\n",
    "- NDCG@10: 0.2551\n",
    "- Recall@10: 0.4657\n",
    "- MRR@10: 0.1911"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Post Fine-tuning Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model\n",
    "finetuned_model_path = \"./models/unixcoder-finetuned\"\n",
    "finetuned_engine = SearchEngine(model_name=finetuned_model_path)\n",
    "index_corpus(finetuned_engine, corpus, finetuned_model_path, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run queries\n",
    "finetuned_results = {}\n",
    "for query in queries:\n",
    "    query_id = query[\"query_id\"]\n",
    "    query_text = query[\"query_text\"]\n",
    "    search_results = finetuned_engine.search(query_text, top_k=10)\n",
    "    retrieved_indices = [r[\"id\"] for r in search_results]\n",
    "    finetuned_results[query_id] = retrieved_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fine-tuned metrics\n",
    "finetuned_metrics = calculate_metrics(finetuned_results, relevance)\n",
    "\n",
    "log.info(\"=\"*50)\n",
    "log.info(\"FINE-TUNED MODEL RESULTS\".center(50))\n",
    "log.info(\"=\"*50)\n",
    "for metric_name, value in finetuned_metrics.items():\n",
    "    log.info(f\"  {metric_name.upper():<20} {value:.4f}\")\n",
    "log.info(\"=\"*50)\n",
    "\n",
    "# Expected results:\n",
    "# RECALL@10: 0.4260\n",
    "# MRR@10: 0.1621\n",
    "# NDCG@10: 0.2234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bonus 1 - Function Names Only Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model using only function names extracted from code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bonus.extractor import extract_function_name\n",
    "\n",
    "# Extract function names\n",
    "corpus_names = [extract_function_name(code) for code in corpus]\n",
    "\n",
    "# Evaluate with function names\n",
    "engine_names = SearchEngine(model_name=finetuned_model_path)\n",
    "engine_names.index_documents(corpus_names, show_progress=True)\n",
    "\n",
    "results_names = {}\n",
    "for query in queries:\n",
    "    query_id = query[\"query_id\"]\n",
    "    query_text = query[\"query_text\"]\n",
    "    search_results = engine_names.search(query_text, top_k=10)\n",
    "    retrieved_indices = [r[\"id\"] for r in search_results]\n",
    "    results_names[query_id] = retrieved_indices\n",
    "\n",
    "metrics_names = calculate_metrics(results_names, relevance)\n",
    "\n",
    "log.info(\"=\"*50)\n",
    "log.info(\"FUNCTION NAMES ONLY RESULTS\".center(50))\n",
    "log.info(\"=\"*50)\n",
    "for metric_name, value in metrics_names.items():\n",
    "    log.info(f\"  {metric_name.upper():<20} {value:.4f}\")\n",
    "log.info(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus 2 - Similarity Metrics Comparison\n",
    "\n",
    "Compare different similarity metrics (Cosine, Euclidean, Manhattan, Dot Product) with both normalized and unnormalized embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bonus.bonus2 import run_all_metrics\n",
    "\n",
    "# This will evaluate all combinations of:\n",
    "# - Metrics: cosine, euclidean, manhattan, dot_product\n",
    "# - Normalization: normalized, unnormalized\n",
    "run_all_metrics()\n",
    "\n",
    "# Results saved to: bonus/results/similarity_metrics.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2 Results Summary\n",
    "\n",
    "**Best Performance: Manhattan Distance with Normalized Embeddings**\n",
    "- Recall@10: 0.4240\n",
    "- MRR@10: 0.1658\n",
    "- NDCG@10: 0.2261\n",
    "\n",
    "**Normalized Embeddings:**\n",
    "- Cosine: Recall@10: 0.3860, MRR@10: 0.1556, NDCG@10: 0.2092\n",
    "- Euclidean: Recall@10: 0.3860, MRR@10: 0.1513, NDCG@10: 0.2060\n",
    "- Manhattan: Recall@10: 0.4240, MRR@10: 0.1658, NDCG@10: 0.2261\n",
    "- Dot Product: Recall@10: 0.3860, MRR@10: 0.1556, NDCG@10: 0.2092\n",
    "\n",
    "**Unnormalized Embeddings:**\n",
    "- Cosine: Recall@10: 0.3860, MRR@10: 0.1579, NDCG@10: 0.2109\n",
    "- Euclidean: Recall@10: 0.3200, MRR@10: 0.1274, NDCG@10: 0.1723\n",
    "- Manhattan: Recall@10: 0.3400, MRR@10: 0.1337, NDCG@10: 0.1817\n",
    "- Dot Product: Recall@10: 0.3840, MRR@10: 0.1457, NDCG@10: 0.2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embeddings Analysis - PCA and Anisotropy\n",
    "\n",
    "Analyze the structure of fine-tuned embeddings to understand their intrinsic dimensionality and detect anisotropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load fine-tuned normalized embeddings\n",
    "with open('cache/embeddings/embeddings_finetuned_normalized.pkl.npz', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Total values: {embeddings.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity Analysis\n",
    "exact_zeros = np.sum(embeddings == 0)\n",
    "near_zeros_001 = np.sum(np.abs(embeddings) < 0.001)\n",
    "near_zeros_01 = np.sum(np.abs(embeddings) < 0.01)\n",
    "near_zeros_05 = np.sum(np.abs(embeddings) < 0.05)\n",
    "total = embeddings.size\n",
    "\n",
    "print(\"Sparsity Analysis:\")\n",
    "print(f\"  Exact zeros (= 0):        {exact_zeros:,} ({exact_zeros/total*100:.4f}%)\")\n",
    "print(f\"  Near-zero (< 0.001):      {near_zeros_001:,} ({near_zeros_001/total*100:.4f}%)\")\n",
    "print(f\"  Near-zero (< 0.01):       {near_zeros_01:,} ({near_zeros_01/total*100:.4f}%)\")\n",
    "print(f\"  Near-zero (< 0.05):       {near_zeros_05:,} ({near_zeros_05/total*100:.4f}%)\")\n",
    "\n",
    "print(f\"\\nValue Statistics:\")\n",
    "print(f\"  Min:    {embeddings.min():.6f}\")\n",
    "print(f\"  Max:    {embeddings.max():.6f}\")\n",
    "print(f\"  Mean:   {embeddings.mean():.6f}\")\n",
    "print(f\"  Std:    {embeddings.std():.6f}\")\n",
    "print(f\"  Median: {np.median(embeddings):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis - Intrinsic Dimensionality\n",
    "print(\"Computing PCA...\")\n",
    "pca = PCA()\n",
    "pca.fit(embeddings)\n",
    "\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "dims_50 = np.argmax(cumvar >= 0.50) + 1\n",
    "dims_90 = np.argmax(cumvar >= 0.90) + 1\n",
    "dims_95 = np.argmax(cumvar >= 0.95) + 1\n",
    "dims_99 = np.argmax(cumvar >= 0.99) + 1\n",
    "\n",
    "print(f\"\\nIntrinsic Dimensionality:\")\n",
    "print(f\"  50% variance: {dims_50:3d} / 768 ({dims_50/768*100:.1f}%)\")\n",
    "print(f\"  90% variance: {dims_90:3d} / 768 ({dims_90/768*100:.1f}%)\")\n",
    "print(f\"  95% variance: {dims_95:3d} / 768 ({dims_95/768*100:.1f}%)\")\n",
    "print(f\"  99% variance: {dims_99:3d} / 768 ({dims_99/768*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  Top 10 components explain: {cumvar[9]*100:.2f}% of variance\")\n",
    "print(f\"  Top 50 components explain: {cumvar[49]*100:.2f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Results Summary\n",
    "\n",
    "**Actual Results:**\n",
    "- Shape: (20604, 768)\n",
    "- No exact zeros (0.0000%)\n",
    "- Near-zero (< 0.05): 83.71% of values\n",
    "\n",
    "**Intrinsic Dimensionality:**\n",
    "- 31 dimensions (4.0%) explain 50% of variance\n",
    "- 231 dimensions (30.1%) explain 90% of variance\n",
    "- 349 dimensions (45.4%) explain 95% of variance\n",
    "- 584 dimensions (76.0%) explain 99% of variance\n",
    "\n",
    "**Anisotropy Evidence:**\n",
    "\n",
    "The strong dimensional compression (only 31 dims for 50% variance vs expected ~384 for isotropic) indicates **significant anisotropy**. The embeddings are concentrated along preferred directions in the embedding space, rather than being uniformly distributed across all 768 dimensions.\n",
    "\n",
    "**Value Distribution:**\n",
    "- Range: [-0.32, 0.36]\n",
    "- Mean: 0.0011 (centered near zero)\n",
    "- Std: 0.036\n",
    "- Symmetric distribution around zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Actual results from evaluation\n",
    "baseline_metrics = {\n",
    "    'recall@10': 0.2220,\n",
    "    'mrr@10': 0.0890,\n",
    "    'ndcg@10': 0.1197\n",
    "}\n",
    "\n",
    "finetuned_metrics = {\n",
    "    'recall@10': 0.4260,\n",
    "    'mrr@10': 0.1621,\n",
    "    'ndcg@10': 0.2234\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Baseline': baseline_metrics,\n",
    "    'Fine-tuned': finetuned_metrics\n",
    "})\n",
    "\n",
    "comparison['Improvement'] = comparison['Fine-tuned'] - comparison['Baseline']\n",
    "comparison['Improvement %'] = (comparison['Improvement'] / comparison['Baseline']) * 100\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.plot_training import plot_training_history\n",
    "\n",
    "# Generate training plots\n",
    "plot_training_history(\"./training_history.json\")\n",
    "\n",
    "# This creates:\n",
    "# - training_loss.png\n",
    "# - validation_metrics.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metrics Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_names = list(baseline_metrics.keys())\n",
    "baseline_values = list(baseline_metrics.values())\n",
    "finetuned_values = list(finetuned_metrics.values())\n",
    "\n",
    "x = range(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar([i - width/2 for i in x], baseline_values, width, label='Baseline', color='skyblue')\n",
    "ax.bar([i + width/2 for i in x], finetuned_values, width, label='Fine-tuned', color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Baseline vs Fine-tuned Model Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.upper() for m in metrics_names])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Baseline Performance:**\n",
    "   - Recall@10: 0.2220\n",
    "   - MRR@10: 0.0890\n",
    "   - NDCG@10: 0.1197\n",
    "\n",
    "2. **Fine-tuned Performance:**\n",
    "   - Recall@10: 0.4260 (+92% improvement)\n",
    "   - MRR@10: 0.1621 (+82% improvement)\n",
    "   - NDCG@10: 0.2234 (+87% improvement)\n",
    "\n",
    "3. **Best Configuration (Bonus 2):**\n",
    "   - Manhattan distance with normalized embeddings\n",
    "   - Recall@10: 0.4240\n",
    "   - NDCG@10: 0.2261\n",
    "\n",
    "4. **Training Insights:**\n",
    "   - Early stopping at epoch 8\n",
    "   - Best model from epoch 5\n",
    "   - Consistent validation improvement through epochs 1-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
